version: '3.8'

services:
  backend:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "8002:8002"
    networks:
      - data_pipeline_network
  # ===================
  # MongoDB (数据源)
  # ===================
  mongodb:
  image: mongo:5.0.18
  container_name: mongodb
  ports:
    - "27017:27017"
  environment:
    MONGO_INITDB_ROOT_USERNAME: root
    MONGO_INITDB_ROOT_PASSWORD: example
  volumes:
    - mongo_data:/data/db
  networks:
    - data_pipeline_network

  # ===================
  # Kafka & Zookeeper (消息队列)
  # ===================
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: zookeeper
    ports:
      - "2181:2181"
    networks:
      - data_pipeline_network

  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_ADVERTISED_HOST_NAME: kafka
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_CREATE_TOPICS: "mongodb-changes:1:1"
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - zookeeper
    networks:
      - data_pipeline_network

  # ===================
  # Flink (流处理)
  # ===================
  flink-jobmanager:
    image: flink:1.20.1-scala_2.12-java8
    container_name: flink-jobmanager
    ports:
      - "8081:8081"
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
    command: jobmanager
    networks:
      - data_pipeline_network

  flink-taskmanager:
    image: flink:1.20.1-scala_2.12-java8
    container_name: flink-taskmanager
    depends_on:
      - flink-jobmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=flink-jobmanager
    command: taskmanager
    networks:
      - data_pipeline_network

  # ===================
  # HDFS (存储层)
  # ===================
  namenode:
    image: apache/hadoop:3.3.5
    container_name: namenode
    ports:
      - "9870:9870"
      - "9000:9000"
    environment:
      - CLUSTER_NAME=test
    env_file:
      - ./hadoop.env
    volumes:
      - hadoop_namenode:/hadoop/dfs/name
    networks:
      - data_pipeline_network

  datanode:
    image: apache/hadoop:3.3.5
    container_name: datanode
    depends_on:
      - namenode
    environment:
      SERVICE_PRECONDITION: "namenode:9870"
    env_file:
      - ./hadoop.env
    volumes:
      - hadoop_datanode:/hadoop/dfs/data
    networks:
      - data_pipeline_network

  # # ===================
  # # HBase (存储层)
  # # ===================


  # ===================
  # Flink作业提交服务 (模拟服务)
  # ===================
  flink-job-submitter:
    image: flink:1.20.1-scala_2.12-java8
    container_name: flink-job-submitter
    volumes:
      - ./flink-jobs:/opt/flink-jobs
    depends_on:
      - flink-jobmanager
      - flink-taskmanager
      - kafka
      - namenode
      - hbase-master
    command: >
      bash -c "
        echo 'Waiting for services to start...' &&
        sleep 60 &&
        echo 'Submitting Flink job...' &&
        /opt/flink/bin/flink run -d -c com.example.StreamingJob /opt/flink-jobs/streaming-job.jar
      "
    networks:
      - data_pipeline_network

networks:
  data_pipeline_network:
    driver: bridge

volumes:
  mongo_data:
  hadoop_namenode:
  hadoop_datanode:
